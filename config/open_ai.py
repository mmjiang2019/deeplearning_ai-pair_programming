# Here you can deploy a local ai mode via ollama.
# And then you can use the API to interact with the model via ollama server.
# Here is the steps:
# 1. install ollama
# 2. run ollama serve as a service
# 3. request the model via ollama server with style of open ai
# Some references:
# https://platform.openai.com/docs/guides/embeddings
# https://ollama.ai/docs
# https://github.com/jmorganca/ollama
# https://github.com/jmorganca/ollama/blob/main/docs/api.md
# https://github.com/jmorganca/ollama/blob/main/docs/models.md
# https://github.com/jmorganca/ollama/blob/main/docs/config.md
# https://github.com/jmorganca/ollama/blob

BASE_URL="http://127.0.0.1:11434/v1"
API_KEY="ollama"